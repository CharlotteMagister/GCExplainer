{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleasant-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import DenseGCNConv, GNNExplainer\n",
    "from torch_geometric.nn import global_mean_pool, GlobalAttention\n",
    "\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "from torch_geometric.nn import GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned from: https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharingand \n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        num_hidden_units = 64 \n",
    "        self.conv0 = DenseGCNConv(num_node_features, num_hidden_units)\n",
    "        self.conv1 = DenseGCNConv(num_hidden_units, num_hidden_units)\n",
    "        self.conv2 = DenseGCNConv(num_hidden_units, num_hidden_units)\n",
    "        \n",
    "        self.pool0 = GlobalAttention(Linear(num_hidden_units, 1))\n",
    "        self.pool1 = GlobalAttention(Linear(num_hidden_units, 1))\n",
    "        self.pool2 = GlobalAttention(Linear(num_hidden_units, 1))\n",
    "        \n",
    "        self.lin = Linear(num_hidden_units, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv0(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        _ = self.pool0(x, batch) \n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        _ = self.pool1(x, batch)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        _ = self.pool2(x, batch)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = self.pool3(x, batch)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "global activation_list\n",
    "activation_list = {}\n",
    "\n",
    "def get_activation(idx):\n",
    "    '''Learned from: https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6'''\n",
    "    def hook(model, input, output):\n",
    "        activation_list[idx] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "def register_hooks(model):\n",
    "    # register hooks to extract activations\n",
    "    model.pool0.register_forward_hook(get_activation('pool0'))\n",
    "    model.pool1.register_forward_hook(get_activation('pool1'))\n",
    "    model.pool2.register_forward_hook(get_activation('pool2'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "separated-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    # enter evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in dataloader:\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    \n",
    "    return correct / len(dataloader.dataset)\n",
    "\n",
    "def train(model, train_loader, test_loader, plot=False, save=False):\n",
    "    # register hooks to track activation\n",
    "    model.train()\n",
    "    model = register_hooks(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # list of accuracies\n",
    "    train_accuracies, test_accuracies = list(), list()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        test_loader_iter = iter(test_loader)\n",
    "        \n",
    "        for data in train_loader:        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            G = nx.Graph()\n",
    "            print(data.edge_index.transpose(1,2).shape)\n",
    "            G.add_edges_from(data.edge_index.transpose(1,2))\n",
    "            edge_list = torch.tensor(nx.to_numpy_matrix(G), requires_grad=True, dtype=torch.float)\n",
    "            \n",
    "            out = model(data.x, edge_list, data.batch)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # get accuracy\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "\n",
    "        ## add to list and print\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
    "                  format(epoch, loss, train_acc, test_acc), end = \"\\r\")\n",
    "\n",
    "\n",
    "    # plut accuracy graph\n",
    "    if plot:\n",
    "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
    "        plt.plot(test_accuracies, label=\"Validation accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    for data in full_loader:        \n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), \"models/mutagenicity_model.pkl\")\n",
    "        \n",
    "        with open(\"models/mutagenicity_activation.txt\", 'wb') as file:\n",
    "            pickle.dump(activation_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crazy-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():    \n",
    "    graphs = TUDataset(root='../../data/TUDataset', name='Mutagenicity')\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def preprocess_input_graphs(graphs):\n",
    "    torch.manual_seed(0)\n",
    "    graphs = graphs.shuffle()\n",
    "    \n",
    "    train_idx = int(len(graphs) * 0.8)\n",
    "    train_set = graphs[:train_idx]\n",
    "    test_set = graphs[train_idx:]\n",
    "        \n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "    full_loader = DataLoader(graphs, batch_size=len(graphs), shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, full_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pretty-shame",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-db771dd3ed77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2804f1b9bc22>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, plot, save)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0medge_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "graphs = load_data()\n",
    "train_loader, test_loader, full_loader = preprocess_input_graphs(graphs)\n",
    "\n",
    "num_node_features = graphs.num_node_features\n",
    "num_classes = graphs.num_classes\n",
    "model = GCN(num_node_features, num_classes)\n",
    "\n",
    "load_pretrained = False\n",
    "\n",
    "if load_pretrained:\n",
    "    model.load_state_dict(torch.load(\"models/mutagenicity_model.pkl\"))\n",
    "    model.eval()\n",
    "\n",
    "    with open(\"models/mutagenicity_activation.txt\", 'rb') as file:\n",
    "        activation_list = pickle.loads(file.read())\n",
    "\n",
    "else:\n",
    "    train(model, train_loader, test_loader, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-lending",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Analysis\n",
    "# num_layers = len(activation_list)\n",
    "# labels = next(iter(full_loader)).y\n",
    "# k = 10\n",
    "\n",
    "# path = f\"output/syn1/{k}Clusters\"\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def plot_activation_space(data, labels, activation_type):\n",
    "#     fig, axs = plt.subplots(num_layers, 1, figsize=(30, 30))\n",
    "#     fig.suptitle(f'{activation_type} (coloured by labels)', fontsize=40)\n",
    "\n",
    "#     counter = 0\n",
    "#     for ax in axs:\n",
    "#         item = data[counter]\n",
    "#         ax.set_title(\"Activations of Layer {}\".format(counter), fontsize=20)\n",
    "#         scatter = ax.scatter(item[:,0], item[:,1], c=labels, cmap='rainbow')\n",
    "#         ax.legend(handles=scatter.legend_elements()[0], labels=list(np.unique(labels)), fontsize=15)\n",
    "#         counter += 1\n",
    "        \n",
    "#     plt.savefig(os.path.join(path, \"tsne.png\"))\n",
    "#     plt.show()\n",
    "            \n",
    "\n",
    "# # TSNE conversion\n",
    "# tsne_models = []\n",
    "# tsne_data = []\n",
    "# for key in activation_list:\n",
    "#     activation = torch.squeeze(activation_list[key]).detach().numpy()\n",
    "#     tsne_model = TSNE(n_components=2)\n",
    "#     tsne_models.append(tsne_model)\n",
    "#     tsne_data.append(tsne_model.fit_transform(activation))\n",
    "    \n",
    "# # plot tsne graphs\n",
    "# plot_activation_space(tsne_data, labels, \"TSNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-minnesota",
   "metadata": {},
   "source": [
    "# Perform KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Perform clustering\n",
    "\n",
    "# def plot_clusters(data, clustering_type):\n",
    "#     fig, axs = plt.subplots(num_layers, 1, figsize=(30, 30))\n",
    "#     fig.suptitle(f'{clustering_type} Clustered Layer Activations', fontsize=40)\n",
    "\n",
    "#     counter = 0\n",
    "#     for ax in axs:\n",
    "#         labels, item = data[counter]\n",
    "#         ax.set_title(\"Clustered Activations of Layer {}\".format(counter), fontsize=20)\n",
    "        \n",
    "#         for i in range(k):\n",
    "#             scatter = ax.scatter(item[labels == i,0], item[labels == i,1], label=f'Cluster {i}')\n",
    "        \n",
    "#         ax.legend(fontsize=15, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#         counter += 1\n",
    "        \n",
    "#     plt.savefig(os.path.join(path, f\"{clustering_type}.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "# kmeans_models = []\n",
    "# kmeans_data = []\n",
    "# for item in tsne_data:\n",
    "#     kmeans_model = KMeans(n_clusters=k, random_state=0)\n",
    "#     kmeans = kmeans_model.fit(item)\n",
    "#     kmeans_models.append(kmeans_model)\n",
    "#     kmeans_data.append((kmeans_model.predict(item), item))\n",
    "\n",
    "# plot_clusters(kmeans_data, \"KMeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) get 3 closest nodes of a cluster\n",
    "\n",
    "# def get_top_graphs(top_indices):    \n",
    "#     graphs = []\n",
    "#     color_maps = []\n",
    "    \n",
    "#     df = pd.DataFrame(edges)\n",
    "    \n",
    "#     for idx in top_indices:\n",
    "#         # get neighbours\n",
    "#         neighbours = list()\n",
    "#         neighbours.append(idx)\n",
    "#         num_expansions = 2\n",
    "\n",
    "#         for i in range(0, num_expansions):\n",
    "#             new_neighbours = list()\n",
    "#             for e in edges:\n",
    "#                 if (e[0] in neighbours) or (e[1] in neighbours):\n",
    "#                     new_neighbours.append(e[0])\n",
    "#                     new_neighbours.append(e[1])\n",
    "\n",
    "#             neighbours = neighbours + new_neighbours\n",
    "#             neighbours = list(set(neighbours))\n",
    "\n",
    "#         new_G = nx.Graph()\n",
    "#         df_neighbours = df[(df[0].isin(neighbours)) & (df[1].isin(neighbours))]\n",
    "#         remaining_edges = df_neighbours.to_numpy()\n",
    "#         new_G.add_edges_from(remaining_edges)\n",
    "\n",
    "#         color_map = []\n",
    "#         for node in new_G:\n",
    "#             if node in top_indices:\n",
    "#                 color_map.append('green')\n",
    "#             else: \n",
    "#                 color_map.append('pink')\n",
    "\n",
    "#         color_maps.append(color_map)\n",
    "#         graphs.append(new_G)\n",
    "            \n",
    "#     return graphs, color_maps\n",
    "\n",
    "\n",
    "# def calc_graph_similarity(top_graphs, num_nodes_view):\n",
    "#     top_G = top_graphs[0]\n",
    "    \n",
    "#     if top_G.number_of_nodes() > 20:\n",
    "#         return \"skipping (too many nodes)\"\n",
    "    \n",
    "#     total_score = 0\n",
    "#     for G in top_graphs[1:]:\n",
    "        \n",
    "#         if G.number_of_nodes() > 20:\n",
    "#             return \"skipping (too many nodes)\"\n",
    "        \n",
    "# #         too slow so going for optimised measure\n",
    "# #         total_score += nx.algorithms.similarity.graph_edit_distance(top_G, G)\n",
    "        \n",
    "#         # optimised measure - get minimum\n",
    "#         min_val = float(\"inf\")\n",
    "#         for val in nx.optimize_graph_edit_distance(top_G, G):\n",
    "#             if val < min_val:\n",
    "#                 min_val = val\n",
    "                \n",
    "#         total_score += min_val\n",
    "        \n",
    "#     return total_score / (len(top_graphs) - 1)\n",
    "\n",
    "\n",
    "# def plot_samples(clustering_model, layer, data, clustering_type, output):\n",
    "#     num_nodes_view = 5\n",
    "    \n",
    "#     fig, axes = plt.subplots(k, num_nodes_view, figsize=(30,30))\n",
    "#     fig.suptitle(f'Nearest to {clustering_type} Centroid for Layer {layer}', fontsize=40)\n",
    "\n",
    "#     l = list(range(0, k))\n",
    "\n",
    "#     for i, ax_list in zip(l, axes):        \n",
    "#         # get top graphs\n",
    "#         distances = clustering_model.transform(data)[:, i]\n",
    "#         top_indices = np.argsort(distances)[::][:num_nodes_view]\n",
    "#         top_graphs, color_maps = get_top_graphs(top_indices)\n",
    "        \n",
    "#         for ax, new_G, color_map in zip(ax_list, top_graphs, color_maps):\n",
    "#             nx.draw(new_G, node_color=color_map, with_labels=True, ax=ax)\n",
    "     \n",
    "#     plt.savefig(os.path.join(path, f\"{output}.png\"))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(kmeans_models)):\n",
    "#     kmeans_model = kmeans_models[i]\n",
    "#     data = tsne_data[i]\n",
    "#     plot_samples(kmeans_model, i, data, \"KMeans\", f\"layer{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-product",
   "metadata": {},
   "source": [
    "# Graph Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes_view = 5\n",
    "\n",
    "# def print_graph_similarity_score(clustering_model, data, layer, max_k):\n",
    "#     output = []\n",
    "#     for k_idx in range(max_k):\n",
    "#         top_graphs, color_maps = get_top_graphs(clustering_model, data, k_idx, num_nodes_view)\n",
    "#         score = calc_graph_similarity(top_graphs, num_nodes_view)\n",
    "#         string = f\"Layer {layer} - Cluster {k_idx} - Avg Graph Similarity Score: {score}\"\n",
    "#         print(string)\n",
    "#         output.append(string)\n",
    "        \n",
    "#     return output\n",
    "        \n",
    "        \n",
    "# output_strs = []\n",
    "# for layer_i, (clustering_model, data) in enumerate(zip(kmeans_models, tsne_data)):\n",
    "#     output = print_graph_similarity_score(clustering_model, data, layer_i, k)\n",
    "#     output_strs += output\n",
    "    \n",
    "# with open(os.path.join(path, f\"graph_similarity_scores.txt\"), 'a') as file:\n",
    "#     for s in output_strs:\n",
    "#         f.write(f'{s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-inspection",
   "metadata": {},
   "source": [
    "# Perform Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform clustering\n",
    "# # Learned from: https://medium.com/@sametgirgin/hierarchical-clustering-model-in-5-steps-with-python-6c45087d4318\n",
    "\n",
    "\n",
    "# def plot_dendrograms(data):\n",
    "#     fig, axs = plt.subplots(num_layers, 1, figsize=(30, 30))\n",
    "#     fig.suptitle(f'HC Dendrograms', fontsize=40)\n",
    "\n",
    "#     counter = 0\n",
    "#     for ax in axs:\n",
    "#         item = data[counter]\n",
    "#         ax.set_title(\"Activations of Layer {}\".format(counter), fontsize=20)\n",
    "        \n",
    "#         dendrogram = hierarchy.dendrogram(hierarchy.linkage(item, method='average'), ax=ax)\n",
    "#         ax.set_xlabel(\"Nodes\")\n",
    "#         ax.set_ylabel(\"Euclidean Distances\")\n",
    "#         counter += 1\n",
    "        \n",
    "#     plt.savefig(os.path.join(path, f\"hc_dendrograms.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "# def plot_hierarchical_clusters(data, n_clusters, clustering_type):\n",
    "#     fig, axs = plt.subplots(num_layers, 1, figsize=(30, 30))\n",
    "#     fig.suptitle(f'HC Clustered Layer Activations', fontsize=40)\n",
    "\n",
    "#     counter = 0\n",
    "#     for ax in axs:\n",
    "#         labels, item = data[counter]\n",
    "#         ax.set_title(f\"{clustering_type} Clustered Activations of Layer {counter}\", fontsize=20)\n",
    "        \n",
    "#         for i in range(n_clusters[counter]):\n",
    "#             scatter = ax.scatter(item[labels == i,0], item[labels == i,1], label=f'Cluster {i}')\n",
    "        \n",
    "#         ax.legend(fontsize=15, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#         counter += 1\n",
    "        \n",
    "#     plt.savefig(os.path.join(path, f\"hc.png\"))\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_hierarchical_clustering_samples(x, y_predict, layer):\n",
    "#     clf = NearestCentroid()\n",
    "#     clf.fit(x, y_predict)\n",
    "#     centroids = clf.centroids_\n",
    "    \n",
    "#     res = pairwise_distances(centroids, x)\n",
    "#     res_sorted = np.argsort(res, axis=-1)\n",
    "    \n",
    "#     n = len(centroids)\n",
    "#     num_nodes_view = 5\n",
    "    \n",
    "#     fig, axes = plt.subplots(n, num_nodes_view, figsize=(30,30))\n",
    "#     fig.suptitle(f'Nearest to hierarchical centroid for Layer {layer}', fontsize=40)\n",
    "    \n",
    "#     l = list(range(0, n))\n",
    "    \n",
    "#     for i, ax_list in zip(l, axes):\n",
    "#         distances = res_sorted[i]\n",
    "#         top_indices = distances[:num_nodes_view]\n",
    "#         top_graphs, color_maps = get_top_graphs(top_indices)\n",
    "        \n",
    "#         for ax, new_G, color_map in zip(ax_list, top_graphs, color_maps):\n",
    "#             nx.draw(new_G, node_color=color_map, with_labels=True, ax=ax)\n",
    "\n",
    "#     plt.savefig(os.path.join(path, f\"hc_samples_layer{layer}.png\"))\n",
    "#     plt.show()\n",
    "                    \n",
    "        \n",
    "# plot_dendrograms(tsne_data)\n",
    "\n",
    "# n_clusters = [5, 7, 8, 9]\n",
    "\n",
    "# hc_models = []\n",
    "# hc_data = []\n",
    "# for i, (item, n) in enumerate(zip(tsne_data, n_clusters)):\n",
    "#     hc = AgglomerativeClustering(n_clusters=n, affinity='euclidean', linkage='average')\n",
    "#     y_hc = hc.fit_predict(item)\n",
    "#     hc_models.append(hc)\n",
    "#     hc_data.append((y_hc, item))\n",
    "#     plot_hierarchical_clustering_samples(item, y_hc, i)\n",
    "\n",
    "# plot_hierarchical_clusters(hc_data, n_clusters, \"HC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-scholarship",
   "metadata": {},
   "source": [
    "# Activation to Concept to Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ActivationClassifier:\n",
    "#     def __init__(self, tsne_data, clustering_model, x, y, edge_list, layer):\n",
    "#         self.tsne_data = tsne_data\n",
    "#         self.clustering_model = clustering_model\n",
    "#         self.x = x.detach().numpy()\n",
    "#         self.y = y.detach().numpy()\n",
    "#         self.edge_list = edge_list\n",
    "#         self.layer = layer\n",
    "        \n",
    "#         self.decision_tree, self.accuracy = self._train_decision_tree()\n",
    "        \n",
    "        \n",
    "#     def _train_decision_tree(self):\n",
    "#         concepts = []\n",
    "#         for node_idx in range(len(node_data_x)):\n",
    "#             concepts.append(self.activation_to_concept(node_idx))\n",
    "            \n",
    "#         decision_tree = tree.DecisionTreeClassifier()\n",
    "#         decision_tree = decision_tree.fit(concepts, self.y)\n",
    "        \n",
    "#         # decision tree accuracy\n",
    "#         accuracy = decision_tree.score(concepts, self.y)\n",
    "\n",
    "#         return decision_tree, accuracy\n",
    "    \n",
    "    \n",
    "#     def get_decision_tree_accuracy(self):\n",
    "#         return self.accuracy\n",
    "    \n",
    "\n",
    "#     def _activation_to_cluster(self, node):\n",
    "#         # apply tsne\n",
    "#         if isinstance(self.clustering_model, KMeans):\n",
    "#             activation = tsne_data[self.layer][node]\n",
    "#             activation = activation.reshape((1, 2))\n",
    "#             cluster = self.clustering_model.predict(activation)\n",
    "#         elif isinstance(self.clustering_model, AgglomerativeClustering):\n",
    "#             cluster = np.array([y_hc[node]])\n",
    "\n",
    "#         return cluster\n",
    "\n",
    "    \n",
    "#     def _cluster_to_concept(self, cluster):\n",
    "#         concept = cluster\n",
    "\n",
    "#         return concept\n",
    "\n",
    "\n",
    "#     def activation_to_concept(self, node):\n",
    "#         # get cluster for node\n",
    "#         cluster = self._activation_to_cluster(node)\n",
    "\n",
    "#         # return cluster number as substitute of concept\n",
    "#         concept = self._cluster_to_concept(cluster)\n",
    "\n",
    "#         return concept\n",
    "\n",
    "    \n",
    "#     def concept_to_class(self, concept):\n",
    "#         concept = concept.reshape(1, -1)\n",
    "#         cls = self.decision_tree.predict(concept)\n",
    "\n",
    "#         return cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-consortium",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get data\n",
    "# node_data_x = torch.from_numpy(features).float()\n",
    "# node_data_y = torch.from_numpy(labels)\n",
    "# edge_list = torch.from_numpy(edges).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-tyler",
   "metadata": {},
   "source": [
    "### Using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# chosen_layer = len(activation_list) - 1\n",
    "# KmeansActivationCls = ActivationClassifier(tsne_data, kmeans_models[chosen_layer], node_data_x, node_data_y, edge_list, chosen_layer)\n",
    "\n",
    "# print(\"Decision Tree Accuracy: \", KmeansActivationCls.get_decision_tree_accuracy())\n",
    "\n",
    "# # activation to concept\n",
    "# node_idx = 224\n",
    "# concept = KmeansActivationCls.activation_to_concept(node_idx)\n",
    "# print(\"Predicted concept is: \", concept)\n",
    "\n",
    "# cls = KmeansActivationCls.concept_to_class(concept)\n",
    "# print(\"Predicted class is: \", cls ,\" where real one is: \", node_data_y[node_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-injection",
   "metadata": {},
   "source": [
    "### Using Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vars\n",
    "# chosen_layer = len(activation_list) - 1\n",
    "# HcActivationCls = ActivationClassifier(tsne_data, hc, node_data_x, node_data_y, edge_list, chosen_layer)\n",
    "\n",
    "# print(\"Decision Tree Accuracy: \", HcActivationCls.get_decision_tree_accuracy())\n",
    "\n",
    "# # activation to concept\n",
    "# node_idx = 224\n",
    "# concept = HcActivationCls.activation_to_concept(node_idx)\n",
    "# print(\"Predicted concept is: \", concept)\n",
    "\n",
    "# cls = HcActivationCls.concept_to_class(concept)\n",
    "# print(\"Predicted class is: \", cls ,\" where real one is: \", node_data_y[node_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-float",
   "metadata": {},
   "source": [
    "# GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import copy\n",
    "# from math import sqrt\n",
    "# from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import networkx as nx\n",
    "# from torch_geometric.nn import MessagePassing\n",
    "# from torch_geometric.data import Data\n",
    "# from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "\n",
    "# EPS = 1e-15\n",
    "\n",
    "# class GNNExplainer2(torch.nn.Module):\n",
    "#     r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "#     Explanations for Graph Neural Networks\"\n",
    "#     <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "#     structures and small subsets node features that play a crucial role in a\n",
    "#     GNNâ€™s node-predictions.\n",
    "\n",
    "#     .. note::\n",
    "\n",
    "#         For an example of using GNN-Explainer, see `examples/gnn_explainer.py\n",
    "#         <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "#         gnn_explainer.py>`_.\n",
    "\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The GNN module to explain.\n",
    "#         epochs (int, optional): The number of epochs to train.\n",
    "#             (default: :obj:`100`)\n",
    "#         lr (float, optional): The learning rate to apply.\n",
    "#             (default: :obj:`0.01`)\n",
    "#         num_hops (int, optional): The number of hops the :obj:`model` is\n",
    "#             aggregating information from.\n",
    "#             If set to :obj:`None`, will automatically try to detect this\n",
    "#             information based on the number of\n",
    "#             :class:`~torch_geometric.nn.conv.message_passing.MessagePassing`\n",
    "#             layers inside :obj:`model`. (default: :obj:`None`)\n",
    "#         return_type (str, optional): Denotes the type of output from\n",
    "#             :obj:`model`. Valid inputs are :obj:`\"log_prob\"` (the model returns\n",
    "#             the logarithm of probabilities), :obj:`\"prob\"` (the model returns\n",
    "#             probabilities) and :obj:`\"raw\"` (the model returns raw scores).\n",
    "#             (default: :obj:`\"log_prob\"`)\n",
    "#         log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "#             progress. (default: :obj:`True`)\n",
    "#     \"\"\"\n",
    "\n",
    "#     coeffs = {\n",
    "#         'edge_size': 0.005,\n",
    "#         'edge_reduction': 'sum',\n",
    "#         'node_feat_size': 1.0,\n",
    "#         'node_feat_reduction': 'mean',\n",
    "#         'edge_ent': 1.0,\n",
    "#         'node_feat_ent': 0.1,\n",
    "#     }\n",
    "\n",
    "#     def __init__(self, model, epochs: int = 100, lr: float = 0.01,\n",
    "#                  num_hops: Optional[int] = None, return_type: str = 'log_prob',\n",
    "#                  log: bool = True):\n",
    "#         super(GNNExplainer2, self).__init__()\n",
    "#         assert return_type in ['log_prob', 'prob', 'raw']\n",
    "#         self.model = model\n",
    "#         self.epochs = epochs\n",
    "#         self.lr = lr\n",
    "#         self.__num_hops__ = num_hops\n",
    "#         self.return_type = return_type\n",
    "#         self.log = log\n",
    "\n",
    "#     def __set_masks__(self, x, edge_index, init=\"normal\"):\n",
    "#         (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "#         std = 0.1\n",
    "#         self.node_feat_mask = torch.nn.Parameter(torch.randn(F) * 0.1)\n",
    "\n",
    "#         std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "#         self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "\n",
    "#         for module in self.model.modules():\n",
    "#             if isinstance(module, MessagePassing):\n",
    "#                 module.__explain__ = True\n",
    "#                 module.__edge_mask__ = self.edge_mask\n",
    "\n",
    "#     def __clear_masks__(self):\n",
    "#         for module in self.model.modules():\n",
    "#             if isinstance(module, MessagePassing):\n",
    "#                 module.__explain__ = False\n",
    "#                 module.__edge_mask__ = None\n",
    "#         self.node_feat_masks = None\n",
    "#         self.edge_mask = None\n",
    "\n",
    "#     @property\n",
    "#     def num_hops(self):\n",
    "#         if self.__num_hops__ is not None:\n",
    "#             return self.__num_hops__\n",
    "\n",
    "#         k = 0\n",
    "#         for module in self.model.modules():\n",
    "#             if isinstance(module, MessagePassing):\n",
    "#                 k += 1\n",
    "#         return k\n",
    "\n",
    "#     def __flow__(self):\n",
    "#         for module in self.model.modules():\n",
    "#             if isinstance(module, MessagePassing):\n",
    "#                 return module.flow\n",
    "#         return 'source_to_target'\n",
    "\n",
    "#     def __subgraph__(self, node_idx, x, edge_index, **kwargs):\n",
    "#         num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "\n",
    "#         subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "#             node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "#             num_nodes=num_nodes, flow=self.__flow__())\n",
    "\n",
    "#         x = x[subset]\n",
    "#         for key, item in kwargs.items():\n",
    "#             if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "#                 item = item[subset]\n",
    "#             elif torch.is_tensor(item) and item.size(0) == num_edges:\n",
    "#                 item = item[edge_mask]\n",
    "#             kwargs[key] = item\n",
    "\n",
    "#         return x, edge_index, mapping, edge_mask, kwargs\n",
    "\n",
    "#     def __loss__(self, node_idx, log_logits, pred_label):\n",
    "#         loss = -log_logits[node_idx, pred_label[node_idx]]\n",
    "\n",
    "#         m = self.edge_mask.sigmoid()\n",
    "#         edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "#         loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "#         ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "#         loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "#         m = self.node_feat_mask.sigmoid()\n",
    "#         node_feat_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "#         loss = loss + self.coeffs['node_feat_size'] * node_feat_reduce(m)\n",
    "#         ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "#         loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def __to_log_prob__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = x.log_softmax(dim=-1) if self.return_type == 'raw' else x\n",
    "#         x = x.log() if self.return_type == 'prob' else x\n",
    "#         return x\n",
    "\n",
    "#     def explain_node(self, node_idx, x, edge_index, **kwargs):\n",
    "#         r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "#         crucial role to explain the prediction made by the GNN for node\n",
    "#         :attr:`node_idx`.\n",
    "\n",
    "#         Args:\n",
    "#             node_idx (int): The node to explain.\n",
    "#             x (Tensor): The node feature matrix.\n",
    "#             edge_index (LongTensor): The edge indices.\n",
    "#             **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "#         :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.model.eval()\n",
    "#         self.__clear_masks__()\n",
    "\n",
    "#         num_edges = edge_index.size(1)\n",
    "\n",
    "#         # Only operate on a k-hop subgraph around `node_idx`.\n",
    "#         x, edge_index, mapping, hard_edge_mask, kwargs = self.__subgraph__(\n",
    "#             node_idx, x, edge_index, **kwargs)\n",
    "\n",
    "#         # Get the initial prediction.\n",
    "#         with torch.no_grad():\n",
    "#             out = self.model(x=x, edge_index=edge_index, **kwargs)\n",
    "#             log_logits = self.__to_log_prob__(out)\n",
    "#             pred_label = log_logits.argmax(dim=-1)\n",
    "\n",
    "#         self.__set_masks__(x, edge_index)\n",
    "#         self.to(x.device)\n",
    "\n",
    "#         optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask],\n",
    "#                                      lr=self.lr)\n",
    "\n",
    "#         if self.log:  # pragma: no cover\n",
    "#             pbar = tqdm(total=self.epochs)\n",
    "#             pbar.set_description(f'Explain node {node_idx}')\n",
    "\n",
    "#         for epoch in range(1, self.epochs + 1):\n",
    "#             optimizer.zero_grad()\n",
    "#             h = x * self.node_feat_mask.view(1, -1).sigmoid()\n",
    "#             out = self.model(x=h, edge_index=edge_index, **kwargs)\n",
    "#             log_logits = self.__to_log_prob__(out)\n",
    "#             loss = self.__loss__(mapping, log_logits, pred_label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if self.log:  # pragma: no cover\n",
    "#                 pbar.set_description(f'Explain node {node_idx}, loss {loss.item()}')\n",
    "#                 pbar.update(1)\n",
    "#                 pbar.refresh()\n",
    "\n",
    "#         if self.log:  # pragma: no cover\n",
    "#             pbar.close()\n",
    "\n",
    "#         node_feat_mask = self.node_feat_mask.detach().sigmoid()\n",
    "#         edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "#         edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "#         self.__clear_masks__()\n",
    "\n",
    "#         return node_feat_mask, edge_mask\n",
    "\n",
    "\n",
    "#     def visualize_subgraph(self, node_idx, edge_index, edge_mask, y=None,\n",
    "#                            threshold=None, **kwargs):\n",
    "#         r\"\"\"Visualizes the subgraph around :attr:`node_idx` given an edge mask\n",
    "#         :attr:`edge_mask`.\n",
    "\n",
    "#         Args:\n",
    "#             node_idx (int): The node id to explain.\n",
    "#             edge_index (LongTensor): The edge indices.\n",
    "#             edge_mask (Tensor): The edge mask.\n",
    "#             y (Tensor, optional): The ground-truth node-prediction labels used\n",
    "#                 as node colorings. (default: :obj:`None`)\n",
    "#             threshold (float, optional): Sets a threshold for visualizing\n",
    "#                 important edges. If set to :obj:`None`, will visualize all\n",
    "#                 edges with transparancy indicating the importance of edges.\n",
    "#                 (default: :obj:`None`)\n",
    "#             **kwargs (optional): Additional arguments passed to\n",
    "#                 :func:`nx.draw`.\n",
    "\n",
    "#         :rtype: :class:`matplotlib.axes.Axes`, :class:`networkx.DiGraph`\n",
    "#         \"\"\"\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "#         # Only operate on a k-hop subgraph around `node_idx`.\n",
    "#         subset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n",
    "#             node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "#             num_nodes=None, flow=self.__flow__())\n",
    "\n",
    "#         edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "#         if threshold is not None:\n",
    "#             print(\"threshold set to \", threshold)\n",
    "#             print(torch.max(edge_mask))\n",
    "#             print(edge_mask.shape)\n",
    "#             edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "#             print(torch.max(edge_mask))\n",
    "\n",
    "#         if y is None:\n",
    "#             y = torch.zeros(edge_index.max().item() + 1,\n",
    "#                             device=edge_index.device)\n",
    "#         else:\n",
    "#             y = y[subset].to(torch.float) / y.max().item()\n",
    "\n",
    "#         data = Data(edge_index=edge_index, att=edge_mask, y=y,\n",
    "#                     num_nodes=y.size(0)).to('cpu')\n",
    "#         G = to_networkx(data, node_attrs=['y'], edge_attrs=['att'])\n",
    "#         mapping = {k: i for k, i in enumerate(subset.tolist())}\n",
    "#         G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "#         node_kwargs = copy(kwargs)\n",
    "#         node_kwargs['node_size'] = kwargs.get('node_size') or 800\n",
    "#         node_kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "#         label_kwargs = copy(kwargs)\n",
    "#         label_kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "\n",
    "#         pos = nx.spring_layout(G)\n",
    "#         ax = plt.gca()\n",
    "#         for source, target, data in G.edges(data=True):\n",
    "#             ax.annotate(\n",
    "#                 '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "#                 textcoords='data', arrowprops=dict(\n",
    "#                     arrowstyle=\"->\",\n",
    "#                     alpha=max(data['att'], 0.1),\n",
    "#                     shrinkA=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "#                     shrinkB=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "#                     connectionstyle=\"arc3,rad=0.1\",\n",
    "#                 ))\n",
    "#         nx.draw_networkx_nodes(G, pos, node_color=y.tolist(), **node_kwargs)\n",
    "#         nx.draw_networkx_labels(G, pos, **label_kwargs)\n",
    "\n",
    "#         return ax, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_idx = 572\n",
    "\n",
    "# # convert to edge format\n",
    "# edges = edge_list.transpose(0, 1).t().contiguous()\n",
    "\n",
    "# explainer = GNNExplainer2(model, epochs=200, return_type='log_prob', log=True)\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, node_data_x, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax, G = explainer.visualize_subgraph(node_idx, edges, edge_mask, y=node_data_y, threshold=0.8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-imperial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_env",
   "language": "python",
   "name": "clustering_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
